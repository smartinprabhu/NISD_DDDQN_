{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import gym\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "class DDDQNAgent:\n",
    "    def __init__(self, state_dim, num_actions, discount_factor=0.99, learning_rate=0.0001):\n",
    "        self.discount_factor = discount_factor\n",
    "        self.learning_rate = learning_rate\n",
    "        self.state_dim = state_dim\n",
    "        self.num_actions = num_actions\n",
    "\n",
    "        # Build the policy and target networks\n",
    "        self.policy_net = self.build_network()\n",
    "        self.target_net = self.build_network()\n",
    "        self.target_net.set_weights(self.policy_net.get_weights())\n",
    "\n",
    "        self.optimizer = tf.keras.optimizers.legacy.Adam(learning_rate=self.learning_rate)\n",
    "\n",
    "        self.memory = []\n",
    "\n",
    "    def build_network(self):\n",
    "        inputs = tf.keras.layers.Input(shape=self.state_dim)\n",
    "        x = tf.keras.layers.Conv2D(32, (8, 8), strides=(4, 4), activation='relu')(inputs)\n",
    "        x = tf.keras.layers.Conv2D(64, (4, 4), strides=(2, 2), activation='relu')(x)\n",
    "        x = tf.keras.layers.Conv2D(64, (3, 3), strides=(1, 1), activation='relu')(x)\n",
    "        x = tf.keras.layers.Flatten()(x)\n",
    "        x = tf.keras.layers.Dense(512, activation='relu')(x)\n",
    "\n",
    "        # Advantage stream\n",
    "        adv_stream = tf.keras.layers.Dense(256, activation='relu')(x)\n",
    "        adv_stream = tf.keras.layers.Dense(self.num_actions, activation='linear')(adv_stream)\n",
    "\n",
    "        # Value stream\n",
    "        val_stream = tf.keras.layers.Dense(256, activation='relu')(x)\n",
    "        val_stream = tf.keras.layers.Dense(1, activation='linear')(val_stream)\n",
    "\n",
    "        # Combine the streams to get Q-values\n",
    "        q_values = val_stream + (adv_stream - tf.reduce_mean(adv_stream, axis=1, keepdims=True))\n",
    "\n",
    "        return tf.keras.Model(inputs=inputs, outputs=q_values)\n",
    "\n",
    "    def select_action(self, state, epsilon):\n",
    "        if np.random.rand() < epsilon:\n",
    "            return np.random.choice(self.num_actions)\n",
    "        q_values = self.policy_net.predict(state)\n",
    "        return np.argmax(q_values[0])\n",
    "\n",
    "    def store_transition(self, state, action, reward, next_state, done):\n",
    "        self.memory.append((state, action, reward, next_state, done))\n",
    "\n",
    "    def learn(self, batch_size=128):\n",
    "        if len(self.memory) < batch_size:\n",
    "            return\n",
    "\n",
    "        indices = np.random.choice(len(self.memory), size=batch_size, replace=False)\n",
    "        batch = [self.memory[i] for i in indices]\n",
    "\n",
    "        states, actions, rewards, next_states, dones = zip(*batch)\n",
    "\n",
    "        states = np.vstack(states)\n",
    "        next_states = np.vstack(next_states)\n",
    "\n",
    "        target_q_values = self.target_net.predict(next_states)\n",
    "        double_q_values = rewards + self.discount_factor * np.max(target_q_values, axis=1) * (1 - np.array(dones)) # Convert dones to a NumPy array\n",
    "\n",
    "        with tf.GradientTape() as tape:\n",
    "            q_values = self.policy_net(states)\n",
    "            actions_one_hot = tf.one_hot(actions, self.num_actions, dtype=tf.float32)\n",
    "            selected_action_q_values = tf.reduce_sum(q_values * actions_one_hot, axis=1)\n",
    "            loss = tf.reduce_mean(tf.square(double_q_values - selected_action_q_values))\n",
    "\n",
    "        grads = tape.gradient(loss, self.policy_net.trainable_variables)\n",
    "        self.optimizer.apply_gradients(zip(grads, self.policy_net.trainable_variables))\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_net.set_weights(self.policy_net.get_weights())\n",
    "\n",
    "    def train(self, num_episodes, epsilon_start=1.0, epsilon_end=0.1, epsilon_decay=0.995):\n",
    "        dataset_folder = '/Users/martinprabhu/Downloads/UAVDT/M0205'  # Replace with the actual path to your \"UAVDT\" folder\n",
    "        target_shape = (84, 84)  # Define your target frame size\n",
    "\n",
    "        def load_dataset(dataset_folder):\n",
    "            dataset = []\n",
    "            for root, _, files in os.walk(dataset_folder):\n",
    "                if files:  # Check if the folder contains files (images)\n",
    "                    episode_data = []\n",
    "                    for filename in sorted(files):\n",
    "                        if filename.endswith(\".jpg\"):  # Adjust the file extension as needed\n",
    "                            filepath = os.path.join(root, filename)\n",
    "                            frame = cv2.imread(filepath)\n",
    "                            if frame is not None:\n",
    "                                episode_data.append(frame)\n",
    "                    if episode_data:\n",
    "                        dataset.append(episode_data)\n",
    "            return dataset\n",
    "\n",
    "        def preprocess_frame(frame, target_shape=(84, 84)):\n",
    "            # Resize the frame to the target shape (e.g., 84x84)\n",
    "            frame = cv2.resize(frame, target_shape)\n",
    "\n",
    "            # Convert the frame to grayscale\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)\n",
    "\n",
    "            # Normalize pixel values to the range [0, 1]\n",
    "            frame = frame.astype(np.float32) / 255.0\n",
    "\n",
    "            # Add a batch dimension and a channel dimension\n",
    "            frame = np.expand_dims(frame, axis=0)\n",
    "            frame = np.expand_dims(frame, axis=-1)\n",
    "\n",
    "            # Return the preprocessed state\n",
    "            return frame\n",
    "\n",
    "        def calculate_reward(state, done):\n",
    "            # Define a reward for balancing the pole\n",
    "            reward = 1.0  # Default reward for each time step\n",
    "\n",
    "            # Optionally, you can provide a penalty for episode termination (e.g., pole falling)\n",
    "            if done:\n",
    "                reward = -1.0  # Penalty for episode termination (pole falling)\n",
    "\n",
    "            return reward\n",
    "\n",
    "        for episode_data in load_dataset(dataset_folder):\n",
    "            episode = dataset_folder.index(episode_data)\n",
    "            print(f\"Training on episode: {episode + 1}\")\n",
    "\n",
    "            done = False\n",
    "            total_reward = 0\n",
    "\n",
    "            for frame in episode_data:\n",
    "                state = preprocess_frame(frame, target_shape)\n",
    "                epsilon = max(epsilon_start * (epsilon_decay ** episode), epsilon_end)\n",
    "                action = self.select_action(state, epsilon)\n",
    "\n",
    "                reward = calculate_reward(state, done)  # Pass state and done as arguments\n",
    "\n",
    "                next_state = preprocess_frame(frame, target_shape)  # Replace with actual next frame\n",
    "                self.store_transition(state, action, reward, next_state, done)\n",
    "\n",
    "                self.learn()\n",
    "\n",
    "                self.update_target_network()\n",
    "\n",
    "                total_reward += reward\n",
    "\n",
    "                print(f\"Episode {episode + 1}/{num_episodes}, Total Reward: {total_reward}\")\n",
    "\n",
    "        print(\"Training finished\")\n",
    "\n",
    "# Create the DDDQN agent and train it\n",
    "state_dim = (84, 84, 1)  # Define the state dimension based on your preprocessing\n",
    "num_actions = 2  # Define the number of actions based on your task\n",
    "agent = DDDQNAgent(state_dim, num_actions)\n",
    "agent.train(num_episodes=200)  # Adjust the number of episodes as needed\n",
    "\n",
    "#Save the trained model\n",
    "agent.policy_net.save(\"model.h5\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.1"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
